{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d77f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_collections\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "import accelerate\n",
    "import utils\n",
    "from datasets import get_dataset\n",
    "from dpm_solver_pp import NoiseScheduleVP, DPM_Solver\n",
    "from absl import logging\n",
    "import builtins\n",
    "import einops\n",
    "import libs.autoencoder\n",
    "import libs.clip\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def stable_diffusion_beta_schedule(linear_start=0.00085, linear_end=0.0120, n_timestep=1000):\n",
    "    _betas = (\n",
    "        torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "    )\n",
    "    return _betas.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4248b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(config):\n",
    "    if config.get('benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "\n",
    "    mp.set_start_method('spawn')\n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    accelerate.utils.set_seed(config.seed, device_specific=True)\n",
    "    logging.info(f'Process {accelerator.process_index} using device: {device}')\n",
    "\n",
    "    config.mixed_precision = accelerator.mixed_precision\n",
    "    config = ml_collections.FrozenConfigDict(config)\n",
    "    if accelerator.is_main_process:\n",
    "        utils.set_logger(log_level='info')\n",
    "    else:\n",
    "        utils.set_logger(log_level='error')\n",
    "        builtins.print = lambda *args: None\n",
    "\n",
    "    dataset = get_dataset(**config.dataset)\n",
    "\n",
    "    # 处理单个文本输入\n",
    "    if config.input_text:\n",
    "        # 直接使用提供的文本\n",
    "        prompt = config.input_text\n",
    "        logging.info(f'使用提供的文本: {prompt}')\n",
    "    elif config.input_file:\n",
    "        # 从文件读取单行文本\n",
    "        with open(config.input_file, 'r', encoding='utf-8') as f:\n",
    "            prompt = f.read().strip()\n",
    "        logging.info(f'从文件读取文本: {prompt}')\n",
    "    else:\n",
    "        raise ValueError(\"必须提供 input_text 或 input_file 参数\")\n",
    "\n",
    "    print(f\"处理文本: {prompt}\")\n",
    "\n",
    "    # 初始化CLIP编码器\n",
    "    clip = libs.clip.BertEmbedder(version='michiyasunaga/BioLinkBERT-base', mask=True)\n",
    "    clip.eval()\n",
    "    clip.to(device)\n",
    "            \n",
    "    # 编码单个文本\n",
    "    context, attn_mask = clip.encode(prompt)  # 传入列表，返回批次维度为1的张量\n",
    "    # context = context * attn_mask.unsqueeze(-1).to(context.device) # mask\n",
    "\n",
    "    # 加载神经网络模型\n",
    "    nnet = utils.get_nnet(**config.nnet)\n",
    "    nnet = accelerator.prepare(nnet)\n",
    "    logging.info(f'从 {config.nnet_path} 加载模型')\n",
    "    accelerator.unwrap_model(nnet).load_state_dict(torch.load(config.nnet_path, map_location='cpu'))\n",
    "    nnet.eval()\n",
    "\n",
    "    def cfg_nnet(x, timesteps, context):\n",
    "        _cond = nnet(x, timesteps, context=context)\n",
    "        if config.sample.scale == 0:\n",
    "            print(\"config.sample.scale == 0, 不使用CFG\")\n",
    "            return _cond\n",
    "        _empty_context = torch.tensor(dataset.empty_context, device=device)\n",
    "        _empty_context = einops.repeat(_empty_context, 'L D -> B L D', B=x.size(0))\n",
    "        _uncond = nnet(x, timesteps, context=_empty_context)\n",
    "        return _cond + config.sample.scale * (_cond - _uncond)\n",
    "\n",
    "    # 加载自动编码器\n",
    "    autoencoder = libs.autoencoder.get_model(**config.autoencoder)\n",
    "    autoencoder.to(device)\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def encode(_batch):\n",
    "        return autoencoder.encode(_batch)\n",
    "\n",
    "    @torch.cuda.amp.autocast()\n",
    "    def decode(_batch):\n",
    "        return autoencoder.decode(_batch)\n",
    "\n",
    "    _betas = stable_diffusion_beta_schedule()\n",
    "    N = len(_betas)\n",
    "\n",
    "    logging.info(config.sample)\n",
    "    logging.info(f'mixed_precision={config.mixed_precision}')\n",
    "    logging.info(f'N={N}')\n",
    "\n",
    "    # 确保输出目录存在\n",
    "    os.makedirs(config.output_path, exist_ok=True)\n",
    "\n",
    "    # 生成单个图像\n",
    "    logging.info(\"开始生成图像...\")\n",
    "    \n",
    "    # 创建随机噪声（批次大小为1）\n",
    "    z_init = torch.randn(1, *config.z_shape, device=device)\n",
    "    noise_schedule = NoiseScheduleVP(schedule='discrete', betas=torch.tensor(_betas, device=device).float())\n",
    "\n",
    "    def model_fn(x, t_continuous):\n",
    "        t = t_continuous * N\n",
    "        return cfg_nnet(x, t, context=context)\n",
    "\n",
    "    # 使用DPM求解器进行采样\n",
    "    dpm_solver = DPM_Solver(model_fn, noise_schedule, predict_x0=True, thresholding=False)\n",
    "    z = dpm_solver.sample(z_init, steps=config.sample.sample_steps, eps=1. / N, T=1.)\n",
    "    \n",
    "    # 解码生成的潜在表示\n",
    "    samples = dataset.unpreprocess(decode(z))\n",
    "    \n",
    "    # 保存生成的图像\n",
    "    sample = samples[0] \n",
    "    output_filename = config.get('output_filename', 'generated_image.png')\n",
    "    output_path = os.path.join(config.output_path, output_filename)\n",
    "    save_image(sample, output_path)\n",
    "    \n",
    "    logging.info(f\"图像已保存到: {output_path}\")\n",
    "    print(f\"生成完成！图像保存在: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0468703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import flags\n",
    "from absl import app\n",
    "from ml_collections import config_flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "config_flags.DEFINE_config_file(\n",
    "    \"config\", None, \"训练配置文件\", lock_config=False)\n",
    "flags.mark_flags_as_required([\"config\"])\n",
    "flags.DEFINE_string(\"nnet_path\", None, \"要评估的神经网络模型路径\")\n",
    "flags.DEFINE_string(\"output_path\", None, \"输出图像的路径\")\n",
    "flags.DEFINE_string(\"input_text\", None, \"输入的文本提示（直接指定文本）\")\n",
    "flags.DEFINE_string(\"input_file\", None, \"输入文本文件的路径（从文件读取文本）\")\n",
    "flags.DEFINE_string(\"output_filename\", \"generated_image.png\", \"输出图像的文件名\")\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    config = FLAGS.config\n",
    "    config.nnet_path = FLAGS.nnet_path\n",
    "    config.output_path = FLAGS.output_path\n",
    "    config.input_text = FLAGS.input_text\n",
    "    config.input_file = FLAGS.input_file\n",
    "    config.output_filename = FLAGS.output_filename\n",
    "    \n",
    "    # 验证输入参数\n",
    "    if not FLAGS.input_text and not FLAGS.input_file:\n",
    "        raise ValueError(\"必须提供 --input_text 或 --input_file 参数之一\")\n",
    "    \n",
    "    if FLAGS.input_text and FLAGS.input_file:\n",
    "        logging.warning(\"同时提供了 input_text 和 input_file，将优先使用 input_text\")\n",
    "    \n",
    "    evaluate(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
